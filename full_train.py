import sys
import pickle as pickle
from datetime import datetime

import torch
from transformers import (
    AutoTokenizer,
    EarlyStoppingCallback,
    TrainingArguments,
)

from args import *
from load_data import *
from model import *
from metric import *
from trainer import *
from utils import *


def train(config):  
    seed_everything(config.seed)
    
    # load model and tokenizer
    model_name = config.model['name']
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # 1. load dataset
    # 2. preprocess dataset
    # 3. tokenize dataset
    revision = "69b6010fe9681567b98f9d3d3c70487079183d4b"
    input_format = config.dataloader.get('input_format')
    prompt = config.dataloader.get('prompt')
    type_transform = config.dataloader.get('type_transform')

    train_dataset, train_raw_label = load_train_dataset(
        split=config.dataloader['train_split'],
        revision=revision,
        tokenizer=tokenizer,
        input_format=input_format,
        prompt=prompt,
        type_transform=type_transform,
    )

    train_label = label_to_num(train_raw_label)

    # 4. make Dataset object
    re_train_dataset = REDataset(train_dataset, train_label)

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(device)

    # 5. import model
    # setting model hyperparameter
    model_module = __import__('model', fromlist=[config.model['variant']])
    model_class = getattr(model_module, config.model['variant'])
    # Available customized classes:
<<<<<<< HEAD
    #   BaseREModel, BiLSTMREModel, BiGRUREModel
=======
    #   REBaseModel, REBiLSTMModel, REBiGRUModel
>>>>>>> b9dea57d4fa9a78c968c3e79acebc1a09aac003d
    model = model_class(config, len(tokenizer))

    print(model.model_config)

    model.parameters
    model.to(device)

    # 6. training arguments ì„¤ì •
    ## ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.
    ## https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments ì°¸ê³ í•´ì£¼ì„¸ìš”.
    training_args = TrainingArguments(
        # ê¸°ë³¸ ì„¤ì •
        output_dir=config.trainer['output_dir'],  # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        report_to=('wandb' if config.use_wandb else 'none'),  # wandb ì‚¬ìš© ì—¬ë¶€
        fp16=True,  # 16-bit floating point precision

        # í•™ìŠµ ì„¤ì •
        num_train_epochs=config.trainer['epochs'],  # ì „ì²´ í›ˆë ¨ epoch ìˆ˜
        learning_rate=config.optimizer['lr'],  # learning rate
        weight_decay=config.optimizer['weight_decay'],  # weight decay
        adam_beta2=config.optimizer['adam_beta2'],  # AdamW ì˜µí‹°ë§ˆì´ì €ì˜ beta2 í•˜ì´í¼íŒŒë¼ë¯¸í„°

        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
        per_device_train_batch_size=config.dataloader['batch_size'],  # í›ˆë ¨ ì¤‘ ì¥ì¹˜ ë‹¹ batch size

        # ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
        warmup_ratio=config.lr_scheduler['warmup_ratio'],  # learning rate schedulerì˜ warmup ë¹„ìœ¨

        # ë¡œê¹… ì„¤ì •
        logging_dir=config.trainer['logging_dir'],  # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
        logging_steps=config.trainer['logging_steps'],  # ë¡œê·¸ ì €ì¥ ìŠ¤í…

        load_best_model_at_end=False,
    )

    # 7. trainer ì„¤ì •
    # 8. evaluate í•¨ìˆ˜ ì„¤ì •
    trainer = RETrainer(
        model=model,  # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=re_train_dataset,  # training dataset
        compute_metrics=compute_metrics,  # define metrics function
        loss_cfg=config.loss,
    )

    # 9. train model
    trainer.train()
    # 10. save model
    trainer.save_model(config.trainer['model_dir'])


def main():
    try:
        config_path = sys.argv[1]
    except IndexError:
        config_path = './config.yaml'
    config = parse_arguments(config_path)

    now = datetime.now()
    run_name = f'{config.run_name}_{now}'

    init_wandb(config, run_name)
    train(config)
    alert_wandb(config, run_name, 'finished')


if __name__ == '__main__':
    main()
