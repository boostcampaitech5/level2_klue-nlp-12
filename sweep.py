import sys
import pickle as pickle
import pytz
from datetime import datetime
import wandb

import torch
from transformers import (
    AutoTokenizer,
    EarlyStoppingCallback,
    TrainingArguments,
)

from args import *
from load_data import *
from model import *
from metric import *
from trainer import *
from utils import *

from typing import Any


def main(config) -> None:

    def sweep_train(config: Any = config) -> None:
        
        print(config.wandb)
        return

        wandb.init(
            entity=config.wandb['entity'],
            project=config.wandb['project_name'],
            #name=run_name,
            #config=config,
        )   

        sweep_config = wandb.config

        seed_everything(config.seed)

        # load model and tokenizer
        model_name = config.model['name']
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        # 1. load dataset
        # 2. preprocess dataset
        # 3. tokenize dataset
        revision = config.dataloader['revision']
        input_format = config.dataloader.get('input_format')
        prompt = config.dataloader.get('prompt')
        type_transform = config.dataloader.get('type_transform')

        train_dataset, train_raw_label = load_train_dataset(
            split=config.dataloader['train_split'],
            revision=revision,
            tokenizer=tokenizer,
            input_format=input_format,
            prompt=prompt,
            type_transform=type_transform,
        )
        dev_dataset, dev_raw_label = load_train_dataset(
            split=config.dataloader['valid_split'],
            revision=revision,
            tokenizer=tokenizer,
            input_format=input_format,
            prompt=prompt,
            type_transform=type_transform,
        )

        train_label = label_to_num(train_raw_label)
        dev_label = label_to_num(dev_raw_label)

        # 4. make Dataset object
        re_train_dataset = REDataset(train_dataset, train_label)
        re_dev_dataset = REDataset(dev_dataset, dev_label)

        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print(device)

        # 5. import model
        # setting model hyperparameter
        model_module = __import__('model', fromlist=[config.model['variant']])
        model_class = getattr(model_module, config.model['variant'])
        # Available customized classes:
        #   BaseREModel, BiLSTMREModel, BiGRUREModel
        model = model_class(config, len(tokenizer))

        print(model.model_config)

        model.parameters
        model.to(device)

        # 6. training arguments ì„¤ì •
        ## ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.
        ## https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments ì°¸ê³ í•´ì£¼ì„¸ìš”.
        training_args = TrainingArguments(
            # ê¸°ë³¸ ì„¤ì •
            output_dir=config.trainer['output_dir'],  # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
            report_to=('wandb' if config.use_wandb else 'none'),  # wandb ì‚¬ìš© ì—¬ë¶€
            fp16=True,  # 16-bit floating point precision

            # í•™ìŠµ ì„¤ì •
            num_train_epochs=sweep_config['epochs'],  # ì „ì²´ í›ˆë ¨ epoch ìˆ˜
            learning_rate=sweep_config['lr'],  # learning rate
            weight_decay=config.optimizer['weight_decay'],  # weight decay
            adam_beta2=config.optimizer['adam_beta2'],  # AdamW ì˜µí‹°ë§ˆì´ì €ì˜ beta2 í•˜ì´í¼íŒŒë¼ë¯¸í„°

            # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
            per_device_train_batch_size=sweep_config['batch_size'],  # í›ˆë ¨ ì¤‘ ì¥ì¹˜ ë‹¹ batch size
            per_device_eval_batch_size=sweep_config['batch_size'],  # í‰ê°€ ì¤‘ ì¥ì¹˜ ë‹¹ batch size

            # ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
            warmup_ratio=config.lr_scheduler['warmup_ratio'],  # learning rate schedulerì˜ warmup ë¹„ìœ¨
            # warmup_steps=config.lr_scheduler['warmup_steps'],  # number of warmup steps for learning rate scheduler

            # ë¡œê¹… ì„¤ì •
            logging_dir=config.trainer['logging_dir'],  # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            logging_steps=config.trainer['logging_steps'],  # ë¡œê·¸ ì €ì¥ ìŠ¤í…

            # ëª¨ë¸ ì €ì¥ ì„¤ì •
            save_total_limit=config.trainer['save_total_limit'],  # ì „ì²´ ì €ì¥ ëª¨ë¸ ìˆ˜ ì œí•œ
            save_steps=config.trainer['save_steps'],  # ëª¨ë¸ ì €ì¥ ìŠ¤í…
            save_strategy=config.trainer['save_strategy'],

            # í‰ê°€ ì„¤ì •
            evaluation_strategy=config.trainer['evaluation_strategy'],  # í›ˆë ¨ ì¤‘ í‰ê°€ ì „ëµ
            eval_steps=config.trainer['evaluation_steps'],  # í‰ê°€ ìŠ¤í…
            load_best_model_at_end=True,
        )

        # 7. trainer ì„¤ì •
        # 8. evaluate í•¨ìˆ˜ ì„¤ì •
        trainer = RETrainer(
            model=model,  # the instantiated ğŸ¤— Transformers model to be trained
            args=training_args,  # training arguments, defined above
            train_dataset=re_train_dataset,  # training dataset
            eval_dataset=re_dev_dataset,  # evaluation dataset
            compute_metrics=compute_metrics,  # define metrics function
            # callbacks=([WandbCallback()] if config.use_wandb else []),
            callbacks=[EarlyStoppingCallback(early_stopping_patience=config.trainer['early_stop'])],
            loss_cfg=config.loss,
        )

        # 9. train model
        trainer.train()
        # 10. save model
        trainer.save_model(config.trainer['model_dir'])

    
    sweep_id = wandb.sweep(
        sweep=config.sweep_config
    )

    wandb.agent(
        sweep_id=sweep_id,
        function=sweep_train,
        count=config.wandb['sweep_count']
    )


if __name__ == '__main__':
    try:
        config_path = sys.argv[1]
    except IndexError:
        config_path = './config.yaml'
    config = parse_arguments(config_path)

    now = datetime.now(pytz.timezone('Asia/Seoul'))
    run_name = f'{config.run_name}_{now.strftime("%d-%H-%M")}'

    main(config)