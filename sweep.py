import sys
import pickle as pickle
import pytz
from datetime import datetime
import wandb

import torch
from transformers import (
    AutoTokenizer,
    EarlyStoppingCallback,
    TrainingArguments,
)

from argparse import Namespace

from utils.args import *
from load_data.load_data import *
from model.model import *
from model.metric import *
from trainer.trainer import *
from utils.utils import *

from typing import Any


def main(config: Namespace) -> None:
    """
    Sweep ì´ˆê¸°í™” ë° Wandb sweep agent ì„ ì–¸

    Args:
        config(Namespace): ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ hyperparameterë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ 
    Returns:
        None
    """
    def sweep_train(config: Namespace = config) -> None:   
        """
        Sweep agent ì„ ì–¸ì‹œ functionì— ì „ë‹¬ë˜ëŠ” í•¨ìˆ˜

        Args:
            config(Namespace): ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ hyperparmeterë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        Returns:
            None
        """
        wandb.init(
            entity=config.wandb['entity'],
            project=config.wandb['sweep_project_name']
        )

        sweep_config = wandb.config

        seed_everything(config.seed)

        # load model and tokenizer
        model_name = config.model['name']
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        # 1. load dataset
        # 2. preprocess dataset
        # 3. tokenize dataset
        revision = config.dataloader['revision']
        input_format = sweep_config['input_format']
        prompt = sweep_config['prompt']
        type_transform = sweep_config['type_transform']

        train_dataset, train_raw_label = load_train_dataset(
            split=config.dataloader['train_split'],
            revision=revision,
            tokenizer=tokenizer,
            input_format=input_format,
            prompt=prompt,
            type_transform=type_transform,
        )
        dev_dataset, dev_raw_label = load_train_dataset(
            split=config.dataloader['valid_split'],
            revision=revision,
            tokenizer=tokenizer,
            input_format=input_format,
            prompt=prompt,
            type_transform=type_transform,
        )

        train_label = label_to_num(train_raw_label)
        dev_label = label_to_num(dev_raw_label)

        # 4. make Dataset object
        re_train_dataset = REDataset(train_dataset, train_label)
        re_dev_dataset = REDataset(dev_dataset, dev_label)

        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print(device)

        # 5. import model
        # setting model hyperparameter
        model_module = __import__('model.model', fromlist=[config.model['variant']])
        model_class = getattr(model_module, config.model['variant'])
        # Available customized classes:
        #   BaseREModel, BiLSTMREModel, BiGRUREModel
        model = model_class(config, len(tokenizer))

        print(model.model_config)

        model.parameters
        model.to(device)

        # 6. training arguments ì„¤ì •
        ## ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.
        ## https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments ì°¸ê³ í•´ì£¼ì„¸ìš”.
        training_args = TrainingArguments(
            # ê¸°ë³¸ ì„¤ì •
            output_dir=config.trainer['output_dir'],  # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
            report_to=('wandb' if config.use_wandb else 'none'),  # wandb ì‚¬ìš© ì—¬ë¶€
            fp16=True,  # 16-bit floating point precision

            # í•™ìŠµ ì„¤ì •
            num_train_epochs=sweep_config['epochs'],  # ì „ì²´ í›ˆë ¨ epoch ìˆ˜
            learning_rate=sweep_config['lr'],  # learning rate
            weight_decay=config.optimizer['weight_decay'],  # weight decay
            adam_beta2=sweep_config['adam_beta2'],  # AdamW ì˜µí‹°ë§ˆì´ì €ì˜ beta2 í•˜ì´í¼íŒŒë¼ë¯¸í„°

            # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
            per_device_train_batch_size=config.dataloader['batch_size'],  # í›ˆë ¨ ì¤‘ ì¥ì¹˜ ë‹¹ batch size
            per_device_eval_batch_size=config.dataloader['batch_size'],  # í‰ê°€ ì¤‘ ì¥ì¹˜ ë‹¹ batch size

            # ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
            warmup_ratio=sweep_config['warmup_ratio'],  # learning rate schedulerì˜ warmup ë¹„ìœ¨
            # warmup_steps=config.lr_scheduler['warmup_steps'],  # number of warmup steps for learning rate scheduler

            # ë¡œê¹… ì„¤ì •
            logging_dir=config.trainer['logging_dir'],  # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            logging_steps=config.trainer['logging_steps'],  # ë¡œê·¸ ì €ì¥ ìŠ¤í…

            # ëª¨ë¸ ì €ì¥ ì„¤ì •
            save_total_limit=config.trainer['save_total_limit'],  # ì „ì²´ ì €ì¥ ëª¨ë¸ ìˆ˜ ì œí•œ
            save_steps=config.trainer['save_steps'],  # ëª¨ë¸ ì €ì¥ ìŠ¤í…
            save_strategy=config.trainer['save_strategy'],

            # í‰ê°€ ì„¤ì •
            evaluation_strategy=config.trainer['evaluation_strategy'],  # í›ˆë ¨ ì¤‘ í‰ê°€ ì „ëµ
            eval_steps=config.trainer['evaluation_steps'],  # í‰ê°€ ìŠ¤í…
            load_best_model_at_end=True,
        )

        # 7. trainer ì„¤ì •
        # 8. evaluate í•¨ìˆ˜ ì„¤ì •
        trainer = RETrainer(
            model=model,  # the instantiated ğŸ¤— Transformers model to be trained
            args=training_args,  # training arguments, defined above
            train_dataset=re_train_dataset,  # training dataset
            eval_dataset=re_dev_dataset,  # evaluation dataset
            compute_metrics=compute_metrics,  # define metrics function
            # callbacks=([WandbCallback()] if config.use_wandb else []),
            # callbacks=[EarlyStoppingCallback(early_stopping_patience=config.trainer['early_stop'])],
            loss_cfg=config.loss,
        )

        # 9. train model
        trainer.train()
        # 10. save model
        trainer.save_model(config.trainer['model_dir'])

    sweep_id = wandb.sweep(
        sweep=config.sweep_config
    )

    wandb.agent(
        sweep_id=sweep_id,
        function=sweep_train,
        count=config.wandb['sweep_count']
    )


if __name__ == '__main__':
    try:
        config_path = sys.argv[1]
    except IndexError:
        config_path = './config.yaml'
    config = parse_arguments(config_path)

    now = datetime.now(pytz.timezone('Asia/Seoul'))
    run_name = f'{config.run_name}_{now.strftime("%d-%H-%M")}'

    main(config)