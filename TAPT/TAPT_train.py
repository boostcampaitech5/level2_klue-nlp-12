import sys
import pickle as pickle
from datetime import datetime

import torch
from transformers import (
    AutoTokenizer,
    EarlyStoppingCallback,
    TrainingArguments,
    RobertaForSequenceClassification,
    AutoConfig,
    AutoModelForMaskedLM
)

from args import *
from load_data import *
from model import *
from metric import *
from trainer import *
from utils import *


def train(config):  
    seed_everything(config.seed)
    
    # load model and tokenizer
    model_name = config.model['name']
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # 1. load dataset
    # 2. preprocess dataset
    # 3. tokenize dataset
    revision = "69b6010fe9681567b98f9d3d3c70487079183d4b"
    input_format = config.dataloader.get('input_format')
    prompt = config.dataloader.get('prompt')
    type_transform = config.dataloader.get('type_transform')

    train_dataset, train_raw_label, num_added_tokens = load_train_dataset(
        split=config.dataloader['train_split'],
        revision=revision,
        tokenizer=tokenizer,
        input_format=input_format,
        prompt=prompt,
        type_transform=type_transform,
    )

    train_label = label_to_num(train_raw_label)

    # 4. make Dataset object
    re_train_dataset = REDataset(train_dataset, train_label)

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(device)

    # 5. load TAPT model
    model_config =  AutoConfig.from_pretrained(model_name)
    model_config.num_labels = 30

    # Load your trained masked language model
    lm_model = AutoModelForMaskedLM.from_pretrained('./TAPT/TAPT_pretrained_model/')

    # Create a new sequence classification model
    model = RobertaForSequenceClassification.from_pretrained(model_name, config=model_config)

    # Copy the base Roberta model weights from your trained model
    model.roberta.load_state_dict(lm_model.roberta.state_dict())

    model.parameters
    model.to(device)


    # 6. training arguments ì„¤ì •
    ## ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.
    ## https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments ì°¸ê³ í•´ì£¼ì„¸ìš”.
    training_args = TrainingArguments(
        # ê¸°ë³¸ ì„¤ì •
        output_dir=config.trainer['output_dir'],  # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        report_to=('wandb' if config.use_wandb else 'none'),  # wandb ì‚¬ìš© ì—¬ë¶€
        fp16=True,  # 16-bit floating point precision

        # í•™ìŠµ ì„¤ì •
        num_train_epochs=config.trainer['epochs'],  # ì „ì²´ í›ˆë ¨ epoch ìˆ˜
        learning_rate=config.optimizer['lr'],  # learning rate
        weight_decay=config.optimizer['weight_decay'],  # weight decay
        adam_beta2=config.optimizer['adam_beta2'],  # AdamW ì˜µí‹°ë§ˆì´ì €ì˜ beta2 í•˜ì´í¼íŒŒë¼ë¯¸í„°

        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
        per_device_train_batch_size=config.dataloader['batch_size'],  # í›ˆë ¨ ì¤‘ ì¥ì¹˜ ë‹¹ batch size

        # ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
        warmup_ratio=config.lr_scheduler['warmup_ratio'],  # learning rate schedulerì˜ warmup ë¹„ìœ¨

        # ë¡œê¹… ì„¤ì •
        logging_dir=config.trainer['logging_dir'],  # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
        logging_steps=config.trainer['logging_steps'],  # ë¡œê·¸ ì €ì¥ ìŠ¤í…

        load_best_model_at_end=False,
    )

    # 7. trainer ì„¤ì •
    # 8. evaluate í•¨ìˆ˜ ì„¤ì •
    trainer = RETrainer(
        model=model,  # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=re_train_dataset,  # training dataset
        compute_metrics=compute_metrics,  # define metrics function
        loss_cfg=config.loss,
    )

    # 9. train model
    trainer.train()
    # 10. save model
    trainer.save_model(config.trainer['model_dir'])


def main():
    try:
        config_path = sys.argv[1]
    except IndexError:
        config_path = './config.yaml'
    config = parse_arguments(config_path)

    now = datetime.now()
    run_name = f'{config.run_name}_{now}'

    init_wandb(config, run_name)
    train(config)
    alert_wandb(config, run_name, 'finished')


if __name__ == '__main__':
    main()