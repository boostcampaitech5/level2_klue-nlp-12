import sys
import pickle as pickle
import pytz
from datetime import datetime
from typing import Dict
from omegaconf import DictConfig

import torch
from transformers import (
    AutoTokenizer,
    EarlyStoppingCallback,
    TrainingArguments,
)

from utils.args import *
from load_data.load_data import *
from model.model import *
from model.metric import *
from trainer.trainer import *
from utils.utils import *


def train(config: DictConfig) -> None:  
    """
    ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜
    
    ë‹¤ìŒ í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜í–‰:
        1. ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³  ì „ì²˜ë¦¬ ë° í† í°í™”
        2. ë ˆì´ë¸”ì„ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜
        3. í•™ìŠµ ë° ê°œë°œ ë°ì´í„°ì…‹ì— ëŒ€í•œ Dataset ê°ì²´ë¥¼ ìƒì„±
        4. ì§€ì •ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ í›ˆë ¨ ì¸ì ì„¤ì •
        5. ëª¨ë¸ í•™ìŠµ í›„ ì €ì¥

    Args:
        config (dict): ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  êµ¬ì„± ë§¤ê°œë³€ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
                       dataloader, model, optimizer, trainer êµ¬ì„± í¬í•¨

    Returns:
        None
    """
    seed_everything(config.seed)
    
    # load model and tokenizer
    model_name = config.model['name']
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # 1. load dataset
    # 2. preprocess dataset
    # 3. tokenize dataset
    revision = config.dataloader['revision']
    input_format = config.dataloader.get('input_format')
    prompt = config.dataloader.get('prompt')
    type_transform = config.dataloader.get('type_transform')

    train_dataset, train_raw_label = load_train_dataset(
        split=config.dataloader['train_split'],
        revision=revision,
        tokenizer=tokenizer,
        input_format=input_format,
        prompt=prompt,
        type_transform=type_transform,
    )
    dev_dataset, dev_raw_label = load_train_dataset(
        split=config.dataloader['valid_split'],
        revision=revision,
        tokenizer=tokenizer,
        input_format=input_format,
        prompt=prompt,
        type_transform=type_transform,
    )

    train_label = label_to_num(train_raw_label)
    dev_label = label_to_num(dev_raw_label)

    # 4. make Dataset object
    re_train_dataset = REDataset(train_dataset, train_label)
    re_dev_dataset = REDataset(dev_dataset, dev_label)

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(device)

    # 5. import model
    # setting model hyperparameter
    model_module = __import__('model.model', fromlist=[config.model['variant']])
    model_class = getattr(model_module, config.model['variant'])
    # Available customized classes:
    #   BaseREModel, BiLSTMREModel, BiGRUREModel
    model = model_class(config, len(tokenizer))

    print(model.model_config)

    model.parameters
    model.to(device)

    # 6. training arguments ì„¤ì •
    ## ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.
    ## https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments ì°¸ê³ í•´ì£¼ì„¸ìš”.
    training_args = TrainingArguments(
        # ê¸°ë³¸ ì„¤ì •
        output_dir=config.trainer['output_dir'],  # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        report_to=('wandb' if config.use_wandb else 'none'),  # wandb ì‚¬ìš© ì—¬ë¶€
        fp16=True,  # 16-bit floating point precision

        # í•™ìŠµ ì„¤ì •
        num_train_epochs=config.trainer['epochs'],  # ì „ì²´ í›ˆë ¨ epoch ìˆ˜
        learning_rate=config.optimizer['lr'],  # learning rate
        weight_decay=config.optimizer['weight_decay'],  # weight decay
        adam_beta2=config.optimizer['adam_beta2'],  # AdamW ì˜µí‹°ë§ˆì´ì €ì˜ beta2 í•˜ì´í¼íŒŒë¼ë¯¸í„°

        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
        per_device_train_batch_size=config.dataloader['batch_size'],  # í›ˆë ¨ ì¤‘ ì¥ì¹˜ ë‹¹ batch size
        per_device_eval_batch_size=config.dataloader['batch_size'],  # í‰ê°€ ì¤‘ ì¥ì¹˜ ë‹¹ batch size

        # ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
        warmup_ratio=config.lr_scheduler['warmup_ratio'],  # learning rate schedulerì˜ warmup ë¹„ìœ¨
        # warmup_steps=config.lr_scheduler['warmup_steps'],  # number of warmup steps for learning rate scheduler

        # ë¡œê¹… ì„¤ì •
        logging_dir=config.trainer['logging_dir'],  # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
        logging_steps=config.trainer['logging_steps'],  # ë¡œê·¸ ì €ì¥ ìŠ¤í…

        # ëª¨ë¸ ì €ì¥ ì„¤ì •
        save_total_limit=config.trainer['save_total_limit'],  # ì „ì²´ ì €ì¥ ëª¨ë¸ ìˆ˜ ì œí•œ
        save_steps=config.trainer['save_steps'],  # ëª¨ë¸ ì €ì¥ ìŠ¤í…
        save_strategy=config.trainer['save_strategy'],

        # í‰ê°€ ì„¤ì •
        evaluation_strategy=config.trainer['evaluation_strategy'],  # í›ˆë ¨ ì¤‘ í‰ê°€ ì „ëµ
        eval_steps=config.trainer['evaluation_steps'],  # í‰ê°€ ìŠ¤í…
        load_best_model_at_end=config.trainer['use_early_stop'],
    )

    # 7. trainer ì„¤ì •
    # 8. evaluate í•¨ìˆ˜ ì„¤ì •
    trainer = RETrainer(
        model=model,  # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=re_train_dataset,  # training dataset
        eval_dataset=re_dev_dataset,  # evaluation dataset
        compute_metrics=compute_metrics,  # define metrics function
        # callbacks=([WandbCallback()] if config.use_wandb else []),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=config.trainer['early_stop'])],
        loss_cfg=config.loss,
    )

    # 9. train model
    trainer.train()
    # 10. save model
    trainer.save_model(config.trainer['model_dir'])


def main() -> None:
    """
    configë¥¼ ë¶ˆëŸ¬ì˜¤ê³  í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
    ë‹¤ìŒ í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜í–‰:
        1. ì œê³µëœ YAML íŒŒì¼ì—ì„œ êµ¬ì„±ì„ íŒŒì‹±í•˜ê±°ë‚˜ ê¸°ë³¸ íŒŒì¼ì„ ì‚¬ìš©
        2. ì œê³µëœ êµ¬ì„±ìœ¼ë¡œ Weights & Biases (wandb) ì‹¤í–‰ ì´ˆê¸°í™”
        3. train í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘
        4. í•™ìŠµ ì™„ë£Œ í›„ wandbì— ì™„ë£Œ ë©”ì„¸ì§€ ì†¡ì¶œ

    Args:
        None

    Returns:
        None
    """
    try:
        config_path = sys.argv[1]
    except IndexError:
        config_path = './config.yaml'
    config = parse_arguments(config_path)

    now = datetime.now(pytz.timezone('Asia/Seoul'))
    run_name = f'{config.run_name}_{now.strftime("%d-%H-%M")}'

    init_wandb(config, run_name)
    train(config)
    alert_wandb(config, run_name, 'finished')


if __name__ == '__main__':
    main()
